per_device_train_batch_size: 64
per_device_eval_batch_size: 32

num_train_epochs: 5
warmup_ratio: 0.1
learning_rate: 3e-4
lr_scheduler_type: "cosine"

seed: 42

eval_strategy: "epoch"

save_strategy: "epoch"
save_total_limit: 2

log_level: "info"
logging_strategy: "steps"
logging_steps: 3


report_to: "wandb"