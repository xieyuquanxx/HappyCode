{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Users/xyq/developer/happy_code/.venv/.dpo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-19 10:52:48,063] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m please install triton==1.0.0 if you want to use sparse attention\n",
      "Python version is above 3.10, patching the collections module.\n",
      "Python version is above 3.10, patching the collections module.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "     Blip2QFormerConfig,\n",
    "     Blip2QFormerModel,\n",
    " )\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from model.memory_bank_ours.models import VLChatProcessor\n",
    "from model.deepseek_vl.utils.io import load_pil_images\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/data/Users/xyq/developer/happy_code/model_repo/deepseek-vl-1.3b-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_chat_processor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_gpt = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).to(\"cuda:0\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_gpt.qformer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_gpt.qformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qformer_config = Blip2QFormerConfig(encoder_hidden_size=1024, hidden_size=1024, vocab_size=tokenizer.vocab_size, num_attention_heads=16)\n",
    "qformer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qformer = Blip2QFormerModel(qformer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count training parameter of qformer\n",
    "sum(p.numel() for p in qformer.parameters()) # 101M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokens = nn.Parameter(\n",
    "    torch.zeros(1, 32, vl_gpt.qformer_config.hidden_size) # [1,32,hidden_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "      {\n",
    "        \"role\": \"User\",\n",
    "        \"content\": \"Current task: craft_stone_pickaxe\\nBased on current task, historical observations and actions, predict the four actions that masked as <action>.\\n<image_placeholder><a><attack></a><a><attack></a><a><attack></a><a><action></a><a><action></a><a><action></a><a><action></a><a><attack><x>-5.81</x><y>-1.61</y></a><a><attack><x>-10.00</x><y>-1.61</y></a><a><attack><forward><x>-5.81</x><y>0.00</y></a><image_placeholder>\",\n",
    "        \"images\": [\n",
    "          \"/data/Users/xyq/developer/happy_code/data/action_dpo/v1/mc_dataset_v1/craft_stone_pickaxe_1385/craft_stone_pickaxe_1385_frame_34.jpg\",\n",
    "          \"/data/Users/xyq/developer/happy_code/data/action_dpo/v1/mc_dataset_v1/craft_stone_pickaxe_1385/craft_stone_pickaxe_1385_frame_35.jpg\"\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"Assistant\",\n",
    "        \"content\": \"<a><attack></a><a><attack></a><a><attack></a><a><attack></a>\"\n",
    "      }\n",
    "    ]\n",
    "pil_images = load_pil_images(conversation)\n",
    "prepare_inputs = vl_chat_processor(conversations=conversation, images=pil_images, force_batchify=True).to(\n",
    "    vl_gpt.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_inputs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = prepare_inputs[\"pixel_values\"]\n",
    "input_ids = prepare_inputs[\"input_ids\"]\n",
    "images_seq_mask = prepare_inputs[\"images_seq_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, n = pixel_values.shape[0:2]\n",
    "images = rearrange(pixel_values, \"b n c h w -> (b n) c h w\")\n",
    "bs, n, images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_features = vl_gpt.vision_model(images)\n",
    "images_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_embeds = rearrange(images_features, \"(b n) t d -> b (n t) d\", b=bs, n=n)\n",
    "images_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_token_bs = query_tokens.expand(images_embeds.shape[0], -1, -1).to(\"cuda:0\")\n",
    "query_token_bs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_attention_mask = torch.ones(images_embeds.size()[:-1], dtype=torch.long, device=\"cuda:0\")\n",
    "# qformer = qformer.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_token_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_outputs = vl_gpt.qformer(\n",
    "    query_embeds=query_token_bs.to(torch.bfloat16),\n",
    "    encoder_hidden_states=images_embeds.to(torch.bfloat16),\n",
    "    encoder_attention_mask=image_attention_mask,\n",
    "    # output_attentions=output_attentions,\n",
    "    # output_hidden_states=output_hidden_states,\n",
    "    # return_dict=return_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_output = query_outputs[0]\n",
    "query_output.shape # same as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds_to_language = vl_gpt.aligner(query_output)\n",
    "image_embeds_to_language.shape # [1, 32, 2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find first False of m1, m1 is bool tensor\n",
    "images_seq_mask = input_ids==100015\n",
    "all_images_seq = torch.nonzero(images_seq_mask, as_tuple=True)\n",
    "first_32_true_indices = all_images_seq[1][32:]\n",
    "images_seq_mask[all_images_seq[0][32:], first_32_true_indices] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_32_true_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_seq_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids[input_ids < 0] = 0\n",
    "input_ids[input_ids == 100015] = 0\n",
    "input_ids[images_seq_mask] = 100015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_embeds = vl_gpt.language_model.get_input_embeddings()(input_ids)\n",
    "inputs_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_emb_mask = prepare_inputs[\"images_emb_mask\"]\n",
    "torch.sum(images_emb_mask==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [b x n, T2, D]\n",
    "# todo: change to qformer!\n",
    "# images_embeds = self.aligner(self.vision_model(images))\n",
    "\n",
    "# [b x n, T2, D] -> [b, n x T2, D]\n",
    "# [b, n, T2] -> [b, n x T2]\n",
    "images_emb_mask = rearrange(images_emb_mask, \"b n t -> b (n t)\")\n",
    "\n",
    "image_attention_mask = torch.ones(images_embeds.size()[:-1], dtype=torch.long, device=images_embeds.device)\n",
    "\n",
    "query_outputs = qformer(\n",
    "query_embeds=query_tokens,\n",
    "encoder_hidden_states=images_embeds,\n",
    "encoder_attention_mask=image_attention_mask,\n",
    "# output_attentions=output_attentions,\n",
    "# output_hidden_states=output_hidden_states,\n",
    "# return_dict=return_dict,\n",
    ")\n",
    "query_output = query_outputs[0]\n",
    "\n",
    "images_embeds = vl_gpt.aligner(query_output)\n",
    "\n",
    "# [b, T, D]\n",
    "input_ids[input_ids < 0] = 0  # ignore the image embeddings\n",
    "# with torch.cuda.amp.autocast():\n",
    "inputs_embeds = vl_gpt.language_model.get_input_embeddings()(input_ids)\n",
    "\n",
    "# replace with the image embeddings\n",
    "# 只取32个token\n",
    "inputs_embeds[images_seq_mask] = images_embeds[images_emb_mask].to(dtype=inputs_embeds.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'c']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = {\"a\": True, \"b\": False, \"c\": True}\n",
    "\n",
    "keys = list(filter(lambda x: f[x], f))\n",
    "keys"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
